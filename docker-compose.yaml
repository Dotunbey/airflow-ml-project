version: '3.8'
services:
  # 1. THE BRAIN: Airflow
  airflow:
    build: .
    command: airflow standalone
    ports:
      - "8080:8080"
    environment:
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      # This trick tells Airflow to use MinIO when we ask for S3
      AWS_ACCESS_KEY_ID: minio_admin
      AWS_SECRET_ACCESS_KEY: minio_password
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
    depends_on:
      - postgres
      - minio

  # 2. THE MEMORY: Postgres (for Airflow's own metadata)
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  # 3. THE DATA LAKE: MinIO (Fake AWS S3)
  minio:
    image: minio/minio
    ports:
      - "9000:9000" # API Port
      - "9001:9001" # UI Port
    environment:
      MINIO_ROOT_USER: minio_admin
      MINIO_ROOT_PASSWORD: minio_password
    command: server /data --console-address ":9001"

  
  minio-create-bucket:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add myminio http://minio:9000 minio_admin minio_password); do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb myminio/stock-data;
      exit 0;
      "
